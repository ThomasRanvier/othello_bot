I implemented my program in Java, I used the given helper code as reference during my work but I implemented everything in my own way.

\section{The makeMove function}

This is the description of the makeMove function.

I choose to realise the clone of the current position using another function which is directly called from the alphaBeta function, it is then not included in the makeMove function.

The first thing done in the function is to check if the asked move is in the grid boundaries and if it is not a pass move.
Then it sets the cell selected by the move to the owned player.
After that for all eight directions it calls the function checkDirection which returns true if the enemy cells on that direction are being captured.
If it returns true the function fillDirection is called, this will capture the enemy cells in between by replacing them by the owned player.
Once the process is done the player is changed for its opponent.

Here is the pseudocode of that function.

\FloatBarrier
\begin{algorithm}
    \caption{make move}
    \begin{algorithmic}[1]
        \Procedure{makeMove}{$x, y$}
            \If{$x$ and $y$ are in boundaries}
                \State $this.grid[x][y] \gets this.player$
                \For{every direction around the cell as $dir\_x$ and $dir\_y$}
                    \If{$this.checkDirection(x, y, dir\_x, dir\_y)$}
                        \State $this.fillDirection(x, y, dir\_x, dir\_y)$
                    \EndIf
                \EndFor
                \State $this.changePlayer()$
            \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\FloatBarrier

The variables `dir\_x' and `dir\_y' corresponds to the values that one has to add respectivelly to `x' and `y' in order to progress along the corresponding direction.
To access to the eight possible directions we just have to make `dir\_x' and `dir\_y' loop through the tree following values: [-1, 0, 1] and not consider the direction where both `dir\_x' and `dir\_y' are 0.

\section{Alpha Beta implementation}

I implemented the Alpha Beta algorithm following the wikipedia pseudocode.
My code can be found in the AlphaBeta.java file that contains the class which is used to determine the best move to do.

Here is the pseudocode of my implementation adapted to the Othello game.

\FloatBarrier
\begin{algorithm}
    \caption{Alpha Beta}
    \begin{algorithmic}[1]
        \Procedure{alphaBeta}{$initial\_position, depth, alpha, beta$}
            \If{$depth == 0 \lor$ no more valid moves}
                \State \textbf{return} a move with value set to the evaluation
            \EndIf
            \State $maximizing \gets initial\_position.getPlayer() == this.player$
            \State $best\_move \gets$ a default move
            \State $value \gets -\infty$ if $maximizing$, $+\infty$ otherwise
            \ForAll{$move$ from valid moves}
                \State $new\_pos \gets$ a clone of $initial\_position$    
                \State $new\_pos.makeMove(move)$
                \State $res\_move \gets alphaBeta(new\_pos, depth - 1, alpha, beta)$
                \If{$maximizing$}
                    \If{$res\_move.value > value$}
                        \State $value \gets res\_move$
                        \State $move.value \gets value$
                        \State $best\_move \gets move$
                    \EndIf
                    \If{$value \ge beta$}
                        \State $move.value \gets value$
                        \State \textbf{return} $move$
                    \EndIf
                    \State $alpha \gets max(value, alpha)$
                \Else
                    \If{$res\_move.value < value$}
                        \State $value \gets res\_move$
                        \State $move.value \gets value$
                        \State $best\_move \gets move$
                    \EndIf
                    \If{$value \le alpha$}
                        \State $move.value \gets value$
                        \State \textbf{return} $move$
                    \EndIf
                    \State $beta \gets min(value, beta)$
                \EndIf
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\FloatBarrier

The recursive alphaBeta function returns an Othello move.

When the maximal depth is reached or if no other moves can be performed the function returns a move with a value set to the evaluation value.
The evaluation value is determined by the evaluator which analyses the board of the game and returns a value that is high if the evaluation for the actual player is `good' and lower otherise.
The evaluator uses a heuristic function that will be described later in this report.

The recursive function calls itself back and since when the `makeMove' function is called the player is changed, the function can automatically know if it must maximize or minimize its selection.
Indeed when the function is selected a move for its own player the higher the evaluation score the better, this is why the function maximizes its selection in that case.
On the other hand when it is selecting a move for the opponent if the evaluation score is low it means that it is a good move for the opponent so the function minimizes its selection in that case.

\section{Iterative deepening}

To handle the time limit the solution is to use iterative deepening for the alplaBeta function.
There is an internal timer in the AlphaBeta class, this timer is checked at each iteration through the valid moves in the alphaBeta function and as soon as the time is reached the function throws an exception.

The alphaBeta function is initially called with a depth of 5 (since it is always way under 1 second) and then the depth is incremented by 1 and the function is called again with the new depth.
That process repeats itself until it catches an exception, then the whole process stops and the move saved from the previous depth is returned as best move.
The maximal depth is set at 20, indeed there is no point to try to analyse the game deeper because it would just be a waste of time when the game is near to end.

Here is the pseudocode of the bestMove function which implements the iterative deepening described above and also directly returns a pass move if the game is over.

\FloatBarrier
\begin{algorithm}
    \caption{best move using Alpha Beta}
    \begin{algorithmic}[1]
        \Procedure{bestMove}{$initial\_position$}
            \If{number of valid moves $== 0$}
                \State \textbf{return} a pass move
            \EndIf
            \State $depth \gets 5$
            \State $best\_move \gets$ a default move
            \While{$depth < 20$ $\land$ current time $<$ time limit}
                \State \textbf{try} \{
                    \State $best\_move \gets this.alphaBeta(initial\_position, depth, -\infty, \infty)$
                    \State $depth \gets depth + 1$
                \State \} \textbf{catch} \{\}
            \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\FloatBarrier

\section{Used heuristics}

The heuristics function is what evaluates a board in order to determine if doing a given move is a good idea or not.
It is the part that differenciates the artificial intelligences from one another, indeed a well balanced and adapted heuristics function will perform way better than a naive heuristics function.

However an evaluation function that is too costly can make the Alpha Beta algorithm go less deep in the recursion and can also be hard to balance.

\subsection{Static weights matrix}

To build my evaluation function I first created a grid that contains weights, each weight correponds to a cell of the grid.
Then to evaluate the board I sum the weights values of the cells owned by each player and return the difference between the total of my player and the total of the other one.
By doing so my program could already beat the naive implementation without any problem.

This is the final weights matrix that is used in my program.

\begin{center}
    \begin{tabular}{ c c c c c c c c } 
        20 & -3 & 11 & 8 & 8 & 11 & -3 & 20\\ 
        -3 & -7 & -4 & -1 & -1 & -4 & -7 & -3\\ 
        11 & -4 & 4 & 2 & 2 & 4 & -4 & 11\\ 
        8 & -1 & 2 & 1 & 1 & 2 & -1 & 8\\ 
        8 & -1 & 2 & 1 & 1 & 2 & -1 & 8\\ 
        11 & -4 & 4 & 2 & 2 & 4 & -4 & 11\\ 
        -3 & -7 & -4 & -1 & -1 & -4 & -7 & -3\\ 
        20 & -3 & 11 & 8 & 8 & 11 & -3 & 20\\ 
    \end{tabular}
\end{center}

We can see that the corners have a high value, that is because when a player captures a corner that piece cannot be capturerd back by the other player.
The pieces on the sides also have a high value because once they are captured by a player they are harder to be captured back by the other player since they can only be captured from 2 directions instead of 8 for the interior pieces.
The pieces around the corners have negative weights, indeed to capture a corner it is needed that the opponent capture one of those pieces, so having negative weights in those places make the program avoid capturing them unless it is really worth it.
The negative weights that are not around the corners have the same purpose that is to facilitate the capture of the pieces on the sides.

\subsection{Actual mobility evaluation}

To improve my evaluation function an idea was to consider the possibility of my player future mobility versus the opponent future mobility.
Indeed the more moves are possible to do the more good potential choices will appear for us in the future.
Another point is that the less moves the opponent can do the easier it will be to beat it.

To evaluate the mobility of a player it is very simple, we just have to call the `getValidMoves' function of the position and the size of the returned vector is the number of possible moves that the player will access to.
Then to get the mobility of the opponent we just have to change the position player, call again `getValidMoves' and then switch the players again to return in the initial state of position.

The returned value of the function is just the difference between the mobility of the player and the mobility of its opponent.

\subsection{Pieces stability}

The last heuristic that I added to my complete evaluation function is the stability evaluation.

To check if a given cell of the board is totally stable we would need to build a complex and especially very not efficient function.
We are then in the case where we need to simplify the function to a point where the function is efficient enough not to slow the Alpha Beta algorithm too much but also give good evaluation results.

I decided to consider a piece as stable if it cannot be captured by the opponent by placing a piece adjacent to the cell the very next move and unstable otherwise.
That kind of stability is called edge stability and it is not very costly to compute it.
Indeed it would be too costly to determine if a piece is definitely stable in the game and the results would not be much better anyway.

The final function that determines if a piece is stable follows that pseudocode.

\FloatBarrier
\begin{algorithm}
    \caption{is a stable cell}
    \begin{algorithmic}[1]
        \Procedure{isStableCell}{$grid, size, x, y$}
            \For{every direction around the cell}
                \If{the first neighbour of the cell in that direction is empty}
                    \If{there is an enemy or empty cell in the opposite direction}
                        \State \textbf{return} $false$
                    \EndIf
                \EndIf
            \EndFor
            \State \textbf{return} $true$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\FloatBarrier

I added the stability evaluation to the function that uses the static weights to determine the value of each cell in the grid.
Now when a cell is owned by a player the value of that cell is equal to the static weight plus a bonus if the cell is stable for the next move.
The bonus added to the cell value has a value of 8, which is the value that I found gave the best results.
